JE:  For some reason Mihal's invitation to join as collaborator was going to my junk mail.  
(Maybe the spam algo didn't trust the name IonicFlavor?)  But I am here, and have now had
a chance to read your interesting discussions.  I like how you have responded to one another
and dug deeper into some questions around literacy and ignorance, moral judgment and sentence
length.  Julius's comments on the lack of ability to take a more fine-toothed sweep through
the datasets on sentencing is very much on point. I wonder, though, if there really is enough
data to support anything more than broad hypotheses.  Chloe's note about the large proportion
of women inmates who were young and black is very astute, and Calista makes good related 
observations about the ages of women inmates and their sentence lengths.  A good project might 
be to study the women in ESP.  They were not terribly numerous compared to men, and one could read all the
admissions entries and moral notes on them over a span of say 25 years, and write a very interesting
report.  

Very nice job on this.  Note that you need to use line breaks when editing a text doc in Git!
Otherwise you'll end up with your whole comment on one very very long line.

You guys get 9.5/10 on this.

Jim




CN: 1) Visualization description: I visualized word frequencies in moral instructor’s notes of inmates and created a “Corpus A” with the characteristics Male, Illiterate, All Sobriety Categories, and All Admission Books and “Corpus B” with the characteristics Male, Reads and Writes, All Sobriety Categories, and All Admission Books. I wanted to isolate the literacy categorization and how it influenced the most common words in moral instructor’s notes. In the Comparative Frequencies plot, words such as “affirms”, “god”, “dead” and “parents” appear with near equal frequency in each plot. However, words associated with literate prisoners are generally more positive (“real”, “baptist”, “pious”, “business”, “child”, etc.) while words associated with illiterate prisoners are more negative (“enmity”, “stupid”, “ignorant”, etc.).
2) Relationship between visualization and repo: The word cloud processes all three admissions books in the repo (A, B, and D). It appears to look at the ColumnNote column and pulls out relevant words to determine whether an inmate can read, write, both, or neither. There is also an option for "Reads other language". The other levers that can be changed are "Admission Book" (A, B, D); "Sobriety" (Sober, Drinks, Drinks a Little, Drinks Often); and "Gender" (Female, Male, All). The visualization selects specific inmate entries based on the specifications and then plots commonly-repeated words from moral instructors' notes. A lot of these words, surprisingly, appear to not be articles such as "the", but more unique words such as "baptist" or "real".
3) Dara errors/limitations: The classification of "can read and write" (calling literate for ease) vs illiterate may be a cause for some confusion in the data since it appears it is based off the column called ColumnNote, which doesn’t clearly separate people who are slightly familiar with reading and writing versus people who are very fluent in reading and writing. Also, while some of these words in the Comparative Frequencies may initially appear positive, they don’t capture context (for example, one of the literate inmates was described as “professing to be God”, which is not positive). This means the connotations of these words can't necessarily be assumed. 
4) Additional visualizations + necessary data: It would be interesting to conduct a sentiment analysis on these words and see how they relate on performance assessment of prisoners (i.e. extending sentence based on bad conduct and reducing sentence based on good conduct) and looking at whether there is correlation with these description. For example, the number of sentence extensions / reductions could be plotted in relation to the positivity/negativity of the words moral instructors used to describe inmates. However, this would require additional data/information on the connotation surrounding these common words used to describe the inmates (i.e., the God example above); this might be difficult to conduct textual analysis on but there could be the incorporation of machine learning or human researchers marking whether or not certain words actually have positive or negative connotations. 

CN: 1) I performed an Eastern Apps data visualization on length of inmates’ sentence in months for ages 11-17 (what would be considered “minors” in current terms), then compared sentencing to ages 18-75. I kept all other variables the default (as “all”) to isolate the effects of age on inmate sentence. In general, I found that the max sentence length for two inmates <18 years of age was around 120 months and the minimum looked to be around 5 months, while the maximum sentence for inmates 18+ years old was ~240 months while the minimum was around a few months (difficult to tell from the graph). The distribution of sentence lengths for both age categories are right-skewed. Generally, there are a lot fewer recorded prisoners <18 years old. 
2) The repo data records information from admission books about inmates, and while some of the characterizing information is supposed to be taken in an objective lens (i.e., race, literacy), other pieces of information may be intended to inform recommended prison sentence length. The data records the prisoner’s age, as well as time in and time out. This particular visualization plots the prisoners’ sentences in months by number of prisoners sentenced and the data is segmented by age, all recorded in the Admission Books. 
3) Scanning through the raw data, there are some missing inputs in entries such as age and offense (but not many). This could impact the data, but it doesn’t appear that too many of these entries are missing (so likely not too large of an impact). Also, the age ranges provided on Eastern Apps range from 11-75 years old, making me wonder if there are any older inmates who are being excluded.
4) There are a few reasons why the data might be appearing like this: maybe there was more favorable sentencing of minors (even though they committed the same types of crimes that 18+ year olds committed), or these minors generally committed less severe crimes than adults. After looking through a bit of the raw data, I found some entries with serious offenses (i.e., Eliza Connelly or Conway, 16 years old, sentenced for poisoning); therefore, it might be more likely that there is more favorable sentencing of minors. It would be interesting to combine these data visualizations with qualitative research about sentencing legislation from the court to see if there were actually more favorable terms for younger prisoners/minors. Another interesting data visualization would be plotting the words from the crimes for <18 year old prisoners versus words from crimes for 18+ prisoners and seeing if there's any repeats of words, common words, or perhaps more severe crimes generally being performed by a specific segment of prisoners. This would require the data on type of crime committed, which is available in the Admission Books.

JS - The app 3 data visualization compares different groups, such as gender and country of origin, with the length of sentence they face. 
The admissions book data has a section with how long the sentence of a person is, which is where they got their data. 
The data only shows at a range of sentences for months. You can only look at disparities in sentence between races by a range of months. 
You can't see the difference of races and compare them with a 1 month sentence and a 12 month sentence at the same time. 
It would be interesting to see how multiple factors, such as age and number of convictions, relate to the sentence length of an inmate. 
This data is already available in the admission books.

JS- The app 1 data visualization compares different groups, such as males and females, literate and not literate, etc, with their word frequencies in the Moral Instructor's notes. The data uses the Moral Instructors notes in the repo, and counts the times it was used by a certain group. With so many words, some words are left out and not defined, which can be very limiting. One minor revision, could be to color code the words based on if they are "negative" or "positive" or "neutral". All of these terms are subjective as not everyone may agree that a word is positive and such, but it would be interesting. This would require someone to judge words based on if they think the word is positive, negative, or neutral. It would also be interesting to compare the word usage as it compares to moral instructors. Unfortunately, the data in the repo doesn't have the author of the instructor notes, so this would be rather hard to do. But it would allow us to see if one instructor may have been known to use a particular word. We may want to remove data like this to get a better correlation. It would also be interesting to know how strong the data is correlated. I wished the creators added a correlation coefficient. The red line they have through the data is a nice touch. 

MZ - Looking at the Descriptions section of the first admissions book, the moral instructors were very anti-Catholic. Catholicism is referred to as "popery" six times, and in other places Catholicism is contrasted with "true religion" ("true religion" requires belief in "regeneration" - I'm not familiar enough with Christianity to guess what this means, does anyone else know?). I'm especially interested in Peter Brill, described as "Jew and Catholic. Deranged and dangerous." - I am unsure how someone can be a Jew and a Catholic at the same time.

MZ @JS - Julius, did you find anything interesting when you ran the comparisons? I can't get the code to run on my own machine yet.

CL- I visualized Word Frequencies (app 1) comparing female inmates who were illiterate and drank (Corpus A) with female inmates who were literate and sober (Corpus B), in all three admission books. Corpus A and B shared some similar common words the frequency of the moral instructor's notes such as, "religious", "crime" but they differ from there. Corpus A individuals frequent more words such as "girls", "innocent", "husband", "education", "mother", and "promise." Corpus B individuals frequent words such as "guilty", "licentiousness", "hardened", "parents", and "instruction." There is clearly a bias or certain favorabilty for Corpus A individuals who might be considered better members of society, compared to their illiterate and drinking counterparts in Corpus B. When reviewing the data files in the repo, there were certainly some missing notes from moral instructors that could potentially skew results shown by the data visualization tools. I also noticed that a lare majority of the female inmates detailed in the repo files were young, Black women who committed non-violent crimes (such as larceny and theft.) It could be interesting to include race, age, and even type of crime (violent or not) into the Corpus variables to potentially analyze the moral instructors' biases and observations. I think Chloe's comment on potentially comapring the connoatation of frequently used words in the moral instructors' notes with inmate sentencing (extension or reduction) is useful is gauging the effect of the moral instructors' notes on the inmates experiences in prison. Including age, race, and type of crime in a comparison like this could also maybe expose how instructor bias affected inmates' sentences.

CL- On Sentencing Inmates (App 2), I compared the sentences of women ages 11-30 who committed larceny to women ages 31-75 who committed larceny. I chose to focus on larceny because that was by far the most common offense I saw listed for women in the repo files. I wanted to compare girls and young women with older women because I saw again on the files that the vast majority of female inmates were very young. I found that there were much more young women, rather than older women, being sentenced for larceny. However, the few older women that were senteced served disproportinately longer sentences than those of the younger women. Maybe incorporating app 1 into this data visualizaition could be useful in determining why the older women received longer sentences. I noticed in the files that a good amount of older convicts were described as "hardened" and not willing to comply with religious reform. Could this have something to do with their longer sentences? 

CN @JS: Julius’ notes on App 3 resonate a lot with my own thoughts on future visualizations of how personal biases are impacting sentence lengths. I especially wonder about the relationship between number of convictions and sentence length; my hypothesis would be there is a positive relationship between number of convictions and sentence length. This could likely be visualized by plotting number of convictions against sentence length and see whether there’s an upwards trend; it would also be interesting to see whether this is a more linear trend or not. For example, if number of convictions really does impact sentence length, then is there a huge difference between 7 & 9 prior convictions versus 1 & 5? Perhaps there could be “diminishing marginal returns” at play.

CN @JS: I had the same response as you, Julius, on App 1 — I think color coding is an interesting idea, especially because it gives more of a sense of the types of attitudes that are taken towards inmates with different characteristics. Unfortunately, it’d probably be a bit more complicated on whether certain words are positive or negative because they’re pulled from the notes and might require looking at contextual clues, but with a bit more work, I think that could also allow for some really interesting insights. I also didn’t dive deep into the idea of the Moral Instructors’ personalities themselves—that could definitely be influencing the type of words that are used in the descriptions (maybe some Instructors are “nicer”/use nicer words than others).

MZ @CN: Interestingly enough, I don't think "ignorant" was entirely a negative descriptor - from what I read in Admissions Book A, ignorance of religion/sin specifically was sometimes seen as a mitigating factor. There seems to be a distinction in how they are discussed between people who are ignorant and also "hardened", and those who are ignorant but willing to learn about religion (i.e. Protestantism - I winder what variety specifically? As @JS said, it would be really helpful to know more about the specific Moral Instructors!)

MZ: A further note on Catholicism - if I'm reading the Eastern Apps visualisation correctly, only two women in all three sheets were described as "Catholic". Unfortunately, since the sample size of women was so much smaller, it's hard to draw any conclusions as to why, and the frequency chart isn't working for this word - it's been glitchy for me all day. Even more than new visualisations, I would like to see some updates to this site to make it less buggy. As it stands, the word clouds are entirely unreadable, for one thing.
